{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffe387b6-6695-4d71-bbb7-dc5066041ddc",
   "metadata": {},
   "source": [
    "# Meowlib Preprocessing Example\n",
    "This notebook is an example template of how to use the structured code (\"`meowlib`\") to preprocess your data for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7364bc-595e-47d0-98f7-b9bfb424bd90",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee20958-cbad-40bf-a891-c4157d22be72",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e9a1e-4e6c-4c4a-8195-754392d9d695",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '../../data/raw_data/zenodo.4008297/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a51e1-e13d-4960-afea-354c651b27b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 15324"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490caf9-959a-4fca-9dca-0f556550ca41",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e9394-1d91-49fa-9b6f-b2cad9ba4910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3142848-0995-4e1b-9665-0b23b33e5612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydub\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import check_random_state\n",
    "from torch import Tensor\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed09e0-720c-41d3-b4d3-e9bd0eafeb83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3a6fa5-76d0-49f4-9d10-1bb10559be1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Local files\n",
    "sys.path.append('../../meowlib/')\n",
    "import utils, data_handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9aa94be-1a7c-4547-a1b4-d86c623fbdf0",
   "metadata": {},
   "source": [
    "## Parse Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3d188b-4ffb-4b3c-b707-a3d568617ada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rng = check_random_state(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f525a-6566-4840-97ba-20e8b98cea36",
   "metadata": {},
   "source": [
    "# Exploratory Analysis\n",
    "Here are examples of pre-processing made easy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f1ef3b-0ca5-4101-96d1-990cf9499502",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f5e34c-a382-411c-af6a-da36226fe471",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3b4ba1-0866-49f6-944e-2d62f4274fad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data. The \"extension\" argument is optional.\n",
    "data_fps = utils.discover_data(data_dir, extension='wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9198bb61-917e-42ef-9067-7e21a0d682ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All files\n",
    "audio_segments = []\n",
    "for data_fp in data_fps:\n",
    "    audio = pydub.AudioSegment.from_file(data_fp)\n",
    "    audio_segments.append(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23702b-5079-4197-8567-138545327865",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Random single file\n",
    "i_example = rng.randint(len(audio_segments))\n",
    "audio_example = audio_segments[i_example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0033a2d-3887-402f-96d0-846f7d830100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Formatted as an array\n",
    "samples_list = data_handling.WavLoader().fit_transform(data_fps)\n",
    "width = np.max([sample[0].size for sample in samples_list])\n",
    "samples = np.zeros((len(samples_list), width))\n",
    "for i, sample in enumerate(samples_list):\n",
    "    samples[i, :sample[0].size] = sample[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5806311-3f38-4b8a-a282-62b69b35c5a4",
   "metadata": {},
   "source": [
    "### User Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b4582c-6a4d-4266-9257-749606697532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_data_fp = '../../data/raw_data/zachs_cats/pip_and_chell_wet_food.m4a'\n",
    "user_audio = pydub.AudioSegment.from_file(user_data_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a38a9-8819-4e6a-b54f-c1ded1f170ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# User signal\n",
    "user_arr = np.array(user_audio.get_array_of_samples())\n",
    "user_arr = user_arr / np.iinfo(user_arr.dtype).max\n",
    "user_sig = Tensor(user_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec4d1f1-ab58-40d8-a5b0-a0438798882b",
   "metadata": {},
   "source": [
    "## Consistency Between Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efceadd9-b91e-416c-a311-057b79fa5c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wavs = data_handling.WavLoader().fit_transform(data_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff97e54-54f1-4913-8ceb-ade220904701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_audio_example = np.array(audio_example.get_array_of_samples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4103e9f-bb44-4122-a4ee-92183fcdf5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch_audio, torch_sample_rate = torchaudio.load(data_fps[i_example])\n",
    "torch_audio = np.array(torch_audio[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c8570-85b7-45c0-a28a-bb3a2c6e0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(\n",
    "    np.arange(torch_audio.size),\n",
    "    torch_audio,\n",
    "    alpha = 0.5,\n",
    "    linewidth=10,\n",
    "    label='torchaudio',\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    np.arange(wavs[i_example][0].size),\n",
    "    wavs[i_example][0],\n",
    "    alpha = 0.5,\n",
    "    linewidth=5,\n",
    "    label='WavLoader',\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    np.arange(raw_audio_example.size),\n",
    "    raw_audio_example / np.iinfo(raw_audio_example.dtype).max,\n",
    "    alpha = 0.5,\n",
    "    label='pydub',\n",
    ")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf4ac64-ddbe-430e-9f96-f91b3f6cfbeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extract One-Value-Per-File Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd3d5b-19d2-484a-b75a-f4c3575f3130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attrs = [\n",
    "    'duration_seconds',\n",
    "    'max_dBFS',\n",
    "    'dBFS',\n",
    "    'max_possible_amplitude',\n",
    "    'max',\n",
    "    'frame_rate', \n",
    "    'frame_width',\n",
    "    'rms',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caf60f2-eeb8-4c95-9c35-5d3db819d2c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "for audio in audio_segments:\n",
    "    \n",
    "    # Easily-accessible values\n",
    "    for attr in attrs:\n",
    "        data.setdefault(attr, []).append(getattr(audio, attr))\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af25613d-54a9-45be-8337-fb0ecfa0a5ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Overall stats\n",
    "g = sns.PairGrid(\n",
    "    df,\n",
    "    vars=['duration_seconds', 'max_dBFS', 'dBFS', ],\n",
    ")\n",
    "g.map_diag(sns.histplot)\n",
    "g.map_offdiag(sns.scatterplot)\n",
    "\n",
    "fig = plt.gcf()\n",
    "ax = plt.gca()\n",
    "\n",
    "# Show user audio\n",
    "for ax in fig.axes:\n",
    "    xlabel = ax.get_xlabel()\n",
    "    ylabel = ax.get_ylabel()\n",
    "    \n",
    "    if ylabel == 'Count':\n",
    "        ax.axvline(\n",
    "            getattr(user_audio, xlabel),\n",
    "        )\n",
    "    elif xlabel=='' or ylabel=='':\n",
    "        continue\n",
    "    else:\n",
    "        ax.scatter(\n",
    "            getattr(user_audio, xlabel),\n",
    "            getattr(user_audio, ylabel),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad88f7b-97e5-4b1c-990f-d5c12c640d25",
   "metadata": {},
   "source": [
    "## Get Rolling Sample from User Sample\n",
    "Pure audio, just to show an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27228922-f1ad-45b7-b34d-30a093774223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, let's resample to the right frequency\n",
    "resampler = torchaudio.transforms.Resample(user_audio.frame_rate, audio_example.frame_rate)\n",
    "user_sig = resampler(user_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d2b50-4743-4b69-88ee-4053b9e1efe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the window size\n",
    "window_size_seconds = 4.\n",
    "window_size = int(window_size_seconds * audio_example.frame_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394052cf-86b9-4b71-aa50-6fa41ce295da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "window_spacing_seconds = 1.\n",
    "window_spacing = int(window_spacing_seconds * audio_example.frame_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202139c-cfa3-48ed-8d84-cd665b11e2ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sig_size = user_sig.size()[0]\n",
    "window_centers = np.arange(window_size//2, sig_size - window_size//2, window_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878418b-fd6f-4be3-9484-3b0dae611ea3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the windows\n",
    "user_arr_resampled = np.array(user_sig)\n",
    "X = np.array([user_arr_resampled[j-window_size//2:j+window_size//2] for j in window_centers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c5d07e-2ecf-4e5e-896d-a65c83f5eaec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare to raw training sample\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.hist(\n",
    "    np.log10(np.abs(X).max(axis=1)),\n",
    "    bins=16,\n",
    "    density=True,\n",
    "    alpha=0.6,\n",
    ")\n",
    "\n",
    "ax.hist(\n",
    "    np.log10(np.abs(samples).max(axis=1)),\n",
    "    bins=16,\n",
    "    density=True,\n",
    "    alpha=0.6,\n",
    ")\n",
    "\n",
    "ax.set_xlabel('log10(max)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855b535-37ca-4f1d-8a15-d9537a008abf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare to raw training sample\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.hist(\n",
    "    np.log10(np.abs(X).std(axis=1)),\n",
    "    bins=16,\n",
    "    density=True,\n",
    "    alpha=0.6,\n",
    ")\n",
    "\n",
    "ax.hist(\n",
    "    np.log10(np.abs(samples).std(axis=1)),\n",
    "    bins=16,\n",
    "    density=True,\n",
    "    alpha=0.6,\n",
    ")\n",
    "\n",
    "ax.set_xlabel('log10(std)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b9804d-0410-4cb4-b5e1-626b848e4615",
   "metadata": {},
   "source": [
    "## Preprocessing Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2bc488-5db5-47f3-8f87-3f47e1587d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = data_handling.FFMPEGLoader()\n",
    "specgram_transformer = data_handling.SpecgramTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499f0db-9127-458a-9a9a-9548518c3293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_and_rates = loader.fit_transform(data_fps)\n",
    "specgrams = specgram_transformer.fit_transform(data_and_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006da20-6856-4b94-808b-865aa0742192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_data_and_rates = loader.fit_transform([user_data_fp,])\n",
    "user_specgrams = specgram_transformer.fit_transform(user_data_and_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba895e2e-ba27-4501-89a5-47780362e35b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.imshow(user_specgrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a677c3-0a0b-4a95-bbc2-dec48a5866fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the sample rate of the specgrams\n",
    "spec_rate = np.array([specgrams[i].shape[1] / audio_segments[i].duration_seconds for i in range(len(audio_segments))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e53f2f-8a40-4735-848a-fbd17f193569",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.std(spec_rate) < 0.2, 'Variation in the sample rate is high.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb9b6e6-b34f-447e-9f1d-40ee6393344c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.isclose(user_specgrams[0].shape[1] / user_audio.duration_seconds, np.mean(spec_rate), atol=0.5), \\\n",
    "    'Difference between user and training is high'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4055c1-a53d-4e24-ac41-36855c6fc4bd",
   "metadata": {},
   "source": [
    "## Full Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b4a8ee-b9bb-47f2-95ef-ff91b1ac26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = Pipeline([\n",
    "    ('load', data_handling.FFMPEGLoader()),\n",
    "    ('specgram', data_handling.SpecgramTransformer()),\n",
    "    ('pad', data_handling.PadTransformer()),\n",
    "])\n",
    "preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c53306b-1d84-4cc4-8ae6-35e3470f25a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = preprocessing_pipeline.fit_transform(data_fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd2e176-1a1e-404e-9b15-b5375ba067db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de0cb2-29f1-4215-844d-7b770d760631",
   "metadata": {},
   "source": [
    "## Full User Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b90992a-c9b2-4e3c-9e7b-f51bef1caf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_preprocessing_pipeline = Pipeline([\n",
    "    ('load', data_handling.FFMPEGLoader()),\n",
    "    ('specgram', data_handling.SpecgramTransformer()),\n",
    "    ('split', data_handling.RollingWindowSplitter()),\n",
    "    ('pad', data_handling.PadTransformer()),\n",
    "])\n",
    "user_preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af5b9c-9075-433e-96d5-4613df1e5ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_X = user_preprocessing_pipeline.fit_transform([user_data_fp, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3315f9-a816-4641-b271-da14ec86b76d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037486f6-9546-44e8-b0e0-4f42e02ba017",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert X.shape[1:] == user_X.shape[1:], 'Inconsistent shapes between user and training data.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b2cbd-be2b-481c-a54a-1898fc7b4ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2e496f-0092-4fc7-b76a-d0eb9cdf4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "pydub.exceptions.CouldntEncodeError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
