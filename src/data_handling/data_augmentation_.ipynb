{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "13c2b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from scipy.io import wavfile\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import set_style\n",
    "from scipy.io import wavfile\n",
    "import math, random\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "dde56e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "###functions\n",
    "\n",
    "#time_shift\n",
    "# ----------------------------\n",
    "  # Shifts the signal to the left or right by some percent. Values at the end\n",
    "  # are 'wrapped around' to the start of the transformed signal.\n",
    "  # ----------------------------\n",
    "    \n",
    "## I still have doubt about whether this is making the dataset to completely nonsense.\n",
    "@staticmethod\n",
    "def time_shift(aud, shift_limit):\n",
    "    sig,sr = aud\n",
    "    sig_len = sig.shape[0]\n",
    "    shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "    return (np.roll(sig,shift_amt), sr)\n",
    "\n",
    "\n",
    "## add random noise\n",
    "## add random noise\n",
    "def random_noise(aud):\n",
    "    sig,sr = aud\n",
    "    noise_factor = max(original_audio)/30\n",
    "    white_noise = np.random.randn(len(original_audio)) * noise_factor\n",
    "    white_noise = white_noise.astype(np.float32)\n",
    "    sig_noise = sig + white_noise\n",
    "    return sig_noise,sr\n",
    "\n",
    "##time stretch\n",
    "def time_stretch(aud,strech):\n",
    "    sig,sr = aud\n",
    "    sig_len = sig.shape[0]\n",
    "    stre_perc = strech*(2*np.random.rand()-1) # goes from -1 to 1\n",
    "    \n",
    "    strectch_t = librosa.core.resample(\n",
    "        sig.astype(np.float32), \n",
    "        orig_sr=sr,\n",
    "        target_sr=sr*(1+stre_perc))\n",
    "    \n",
    "    return (strectch_t,sr*(1+stre_perc))\n",
    "\n",
    "# ----------------------------\n",
    "  # Augment the Spectrogram by masking out some sections of it in both the frequency\n",
    "  # dimension (ie. horizontal bars) and the time dimension (vertical bars) to prevent\n",
    "  # overfitting and to help the model generalise better. The masked sections are\n",
    "  # replaced with the mean value.\n",
    "  # ----------------------------\n",
    "    \n",
    "@staticmethod\n",
    "def spectro_augment(spec, max_mask_pct=0.08, n_freq_masks=2, n_time_masks=2):\n",
    "    spec = torch.from_numpy(spec)\n",
    "    n_mels, n_steps = spec.shape\n",
    "    mask_value = spec.mean()\n",
    "    aug_spec = spec\n",
    "\n",
    "    freq_mask_param = max_mask_pct * n_mels\n",
    "    for _ in range(n_freq_masks):\n",
    "        aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "    time_mask_param = max_mask_pct * n_steps\n",
    "    for _ in range(n_time_masks):\n",
    "        aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n",
    "    \n",
    "    \n",
    "    return aug_spec.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "89b0890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## will be used to process augmented audio files\n",
    "def pad_trunc(aud, max_t): ## this function is copied and modified from the link: https://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5\n",
    "    sig, sr = aud\n",
    "    num_rows,sig_len = sig.shape\n",
    "    max_len = max_t ## the maximum length of each spectrogram result\n",
    "\n",
    "    if (sig_len > max_len):\n",
    "      # Truncate the signal to the given length\n",
    "        tru_begin = (sig_len - max_len)//2\n",
    "        tru_end = max_len + tru_begin\n",
    "        sig = sig[:,tru_begin:tru_end]\n",
    "\n",
    "    elif (sig_len < max_len):\n",
    "      # Length of padding to add at the beginning and end of the signal\n",
    "        pad_begin_len = (max_len - sig_len)//2\n",
    "        pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "      # Pad with 0s\n",
    "        pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "        pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "        sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "    return (sig, sr)\n",
    "\n",
    "def spectro_gram(aud, n_mels=128, n_fft=600, hop_len=None):\n",
    "    sig,sr = aud\n",
    "    top_db = 80\n",
    "\n",
    "    # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "    spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "    # Convert to decibels\n",
    "    spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "    return (spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "374b42bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##let's add one time shift; one time stretch; one random noise; one spectro_augment; (separatly)\n",
    "## Here I will produce 3 datasets for each method and save it for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1a23a889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(440, 128, 67)\n"
     ]
    }
   ],
   "source": [
    "##load spectrogram data\n",
    "X = np.load('../../data/processed_data/specgram_db_pad_trunc.npy')\n",
    "print(X.shape)\n",
    "df = pd.read_csv(\"../../data/processed_data/metadata.csv\")\n",
    "category_to_number = {'brushing': 0, 'food': 1, 'isolation': 2}\n",
    "\n",
    "# Create a new column with numerical values based on the mapping\n",
    "df['numerical_situation'] = df['situation'].map(category_to_number)\n",
    "\n",
    "y = df['numerical_situation'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9ec3b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftmask1 = []\n",
    "ftmask1_flat = []\n",
    "ftmask2 = []\n",
    "ftmask2_flat = []\n",
    "ftmask3 = []\n",
    "ftmask3_flat = []\n",
    "for spec in X:\n",
    "    aug1 = spectro_augment(spec, max_mask_pct=0.08, n_freq_masks=2, n_time_masks=2)\n",
    "    aug2 = spectro_augment(spec, max_mask_pct=0.08, n_freq_masks=2, n_time_masks=2)\n",
    "    aug3 = spectro_augment(spec, max_mask_pct=0.08, n_freq_masks=2, n_time_masks=2)\n",
    "    ftmask1.append(aug1)\n",
    "    ftmask1_flat.append(aug1.flatten())\n",
    "    ftmask2.append(aug2)\n",
    "    ftmask2_flat.append(aug2.flatten())\n",
    "    ftmask3.append(aug3)\n",
    "    ftmask3_flat.append(aug2.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144bad9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f19f58c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load audio data\n",
    "data_dir = '../../data/raw_data/zenodo.4008297/'\n",
    "data_fps = glob.glob(os.path.join(data_dir, '*.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "73124063",
   "metadata": {},
   "outputs": [],
   "source": [
    "tshift1 = []\n",
    "tshift2 = []\n",
    "tshift3 = []\n",
    "randnoi1 = []\n",
    "randnoi2 = []\n",
    "randnoi3 = []\n",
    "tstret1 = []\n",
    "tstret2 = []\n",
    "tstret3 = []\n",
    "\n",
    "for data_fp in data_fps:\n",
    "    original_audio,sr = librosa.load(data_fp, sr=None) \n",
    "    tshift1.append(time_shift((original_audio,sr), 0.2))\n",
    "    tshift2.append(time_shift((original_audio,sr), 0.2))\n",
    "    tshift3.append(time_shift((original_audio,sr), 0.2))\n",
    "    randnoi1.append(random_noise((original_audio,sr)))\n",
    "    randnoi2.append(random_noise((original_audio,sr)))\n",
    "    randnoi3.append(random_noise((original_audio,sr)))\n",
    "    tstret1.append(time_stretch(((original_audio,sr)),0.2))\n",
    "    tstret2.append(time_stretch(((original_audio,sr)),0.2))\n",
    "    tstret3.append(time_stretch(((original_audio,sr)),0.2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1b55c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_pt_tsh1 = []\n",
    "spec_pt_flat_tsh1 = [] ##flattened version\n",
    "for aud in tshift1:\n",
    "    sig,sr = aud\n",
    "    #print((torch.tensor(sig).view(1, -1),sr))\n",
    "    audnew = pad_trunc((torch.tensor(sig).view(1, -1),sr),20000) ##pad or truncate it ###Has to transfer a list to a torch tensor of the shape: (1,len(list))\n",
    "    spec = spectro_gram(audnew) ##transform the audo to a spec\n",
    "    spec_pt_tsh1.append(np.array(spec)[0,:,:]) ##save it. Note:transfer tensor to array will lead on one redundant dimension, here we get rid of it\n",
    "    spec_pt_flat_tsh1.append(np.array(spec)[0,:,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f2ef0049",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_pt_tsh2 = []\n",
    "spec_pt_flat_tsh2 = [] ##flattened version\n",
    "for aud in tshift2:\n",
    "    sig,sr = aud\n",
    "    #print((torch.tensor(sig).view(1, -1),sr))\n",
    "    audnew = pad_trunc((torch.tensor(sig).view(1, -1),sr),20000) ##pad or truncate it ###Has to transfer a list to a torch tensor of the shape: (1,len(list))\n",
    "    spec = spectro_gram(audnew) ##transform the audo to a spec\n",
    "    spec_pt_tsh2.append(np.array(spec)[0,:,:]) ##save it. Note:transfer tensor to array will lead on one redundant dimension, here we get rid of it\n",
    "    spec_pt_flat_tsh2.append(np.array(spec)[0,:,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7066b9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_pt_tsh3 = []\n",
    "spec_pt_flat_tsh3 = [] ##flattened version\n",
    "for aud in tshift3:\n",
    "    sig,sr = aud\n",
    "    #print((torch.tensor(sig).view(1, -1),sr))\n",
    "    audnew = pad_trunc((torch.tensor(sig).view(1, -1),sr),20000) ##pad or truncate it ###Has to transfer a list to a torch tensor of the shape: (1,len(list))\n",
    "    spec = spectro_gram(audnew) ##transform the audo to a spec\n",
    "    spec_pt_tsh3.append(np.array(spec)[0,:,:]) ##save it. Note:transfer tensor to array will lead on one redundant dimension, here we get rid of it\n",
    "    spec_pt_flat_tsh3.append(np.array(spec)[0,:,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "420be27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_pt_rd1 = []\n",
    "spec_pt_flat_rd1 = [] ##flattened version\n",
    "for aud in randnoi1:\n",
    "    sig,sr = aud\n",
    "    audnew = pad_trunc((torch.tensor(sig).view(1, -1),sr),20000) ##pad or truncate it ###Has to transfer a list to a torch tensor of the shape: (1,len(list))\n",
    "    spec = spectro_gram(audnew) ##transform the audo to a spec\n",
    "    spec_pt_rd1.append(np.array(spec)[0,:,:]) ##save it. Note:transfer tensor to array will lead on one redundant dimension, here we get rid of it\n",
    "    spec_pt_flat_rd1.append(np.array(spec)[0,:,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "350a3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_pt_rd2 = []\n",
    "spec_pt_flat_rd2 = [] ##flattened version\n",
    "for aud in randnoi2:\n",
    "    sig,sr = aud\n",
    "    audnew = pad_trunc((torch.tensor(sig).view(1, -1),sr),20000) ##pad or truncate it ###Has to transfer a list to a torch tensor of the shape: (1,len(list))\n",
    "    spec = spectro_gram(audnew) ##transform the audo to a spec\n",
    "    spec_pt_rd2.append(np.array(spec)[0,:,:]) ##save it. Note:transfer tensor to array will lead on one redundant dimension, here we get rid of it\n",
    "    spec_pt_flat_rd2.append(np.array(spec)[0,:,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "69b8091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_pt_rd3 = []\n",
    "spec_pt_flat_rd3 = [] ##flattened version\n",
    "for aud in randnoi3:\n",
    "    sig,sr = aud\n",
    "    audnew = pad_trunc((torch.tensor(sig).view(1, -1),sr),20000) ##pad or truncate it ###Has to transfer a list to a torch tensor of the shape: (1,len(list))\n",
    "    spec = spectro_gram(audnew) ##transform the audo to a spec\n",
    "    spec_pt_rd3.append(np.array(spec)[0,:,:]) ##save it. Note:transfer tensor to array will lead on one redundant dimension, here we get rid of it\n",
    "    spec_pt_flat_rd3.append(np.array(spec)[0,:,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "299cfae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_pt_tst1 = []\n",
    "spec_pt_flat_tst1 = [] ##flattened version\n",
    "for aud in tstret1:\n",
    "    sig,sr = aud\n",
    "    #print((torch.tensor(sig).view(1, -1),sr))\n",
    "    audnew = pad_trunc((torch.tensor(sig).view(1, -1),sr),20000) ##pad or truncate it ###Has to transfer a list to a torch tensor of the shape: (1,len(list))\n",
    "    spec = spectro_gram(audnew) ##transform the audo to a spec\n",
    "    spec_pt_tst1.append(np.array(spec)[0,:,:]) ##save it. Note:transfer tensor to array will lead on one redundant dimension, here we get rid of it\n",
    "    spec_pt_flat_tst1.append(np.array(spec)[0,:,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "53ca3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_pt_tst2 = []\n",
    "spec_pt_flat_tst2 = [] ##flattened version\n",
    "for aud in tstret2:\n",
    "    sig,sr = aud\n",
    "    #print((torch.tensor(sig).view(1, -1),sr))\n",
    "    audnew = pad_trunc((torch.tensor(sig).view(1, -1),sr),20000) ##pad or truncate it ###Has to transfer a list to a torch tensor of the shape: (1,len(list))\n",
    "    spec = spectro_gram(audnew) ##transform the audo to a spec\n",
    "    spec_pt_tst2.append(np.array(spec)[0,:,:]) ##save it. Note:transfer tensor to array will lead on one redundant dimension, here we get rid of it\n",
    "    spec_pt_flat_tst2.append(np.array(spec)[0,:,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3b6006d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_pt_tst3 = []\n",
    "spec_pt_flat_tst3 = [] ##flattened version\n",
    "for aud in tstret3:\n",
    "    sig,sr = aud\n",
    "    #print((torch.tensor(sig).view(1, -1),sr))\n",
    "    audnew = pad_trunc((torch.tensor(sig).view(1, -1),sr),20000) ##pad or truncate it ###Has to transfer a list to a torch tensor of the shape: (1,len(list))\n",
    "    spec = spectro_gram(audnew) ##transform the audo to a spec\n",
    "    spec_pt_tst3.append(np.array(spec)[0,:,:]) ##save it. Note:transfer tensor to array will lead on one redundant dimension, here we get rid of it\n",
    "    spec_pt_flat_tst3.append(np.array(spec)[0,:,:].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ea44c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data/processed_data/spec_pt_timeshift1.npy',spec_pt_tsh1)\n",
    "np.save('../../data/processed_data/spec_pt_timeshift1_flatten.npy',spec_pt_flat_tsh1)\n",
    "np.save('../../data/processed_data/spec_pt_timeshift2.npy',spec_pt_tsh2)\n",
    "np.save('../../data/processed_data/spec_pt_timeshift2_flatten.npy',spec_pt_flat_tsh2)\n",
    "np.save('../../data/processed_data/spec_pt_timeshift3.npy',spec_pt_tsh3)\n",
    "np.save('../../data/processed_data/spec_pt_timeshift3_flatten.npy',spec_pt_flat_tsh3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "662d31da",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data/processed_data/spec_pt_randnoise1.npy',spec_pt_rd1)\n",
    "np.save('../../data/processed_data/spec_pt_randnoise1_flatten.npy',spec_pt_flat_rd1)\n",
    "np.save('../../data/processed_data/spec_pt_randnoise2.npy',spec_pt_rd2)\n",
    "np.save('../../data/processed_data/spec_pt_randnoise2_flatten.npy',spec_pt_flat_rd2)\n",
    "np.save('../../data/processed_data/spec_pt_randnoise3.npy',spec_pt_rd3)\n",
    "np.save('../../data/processed_data/spec_pt_randnoise3_flatten.npy',spec_pt_flat_rd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "19950d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data/processed_data/spec_pt_timestretch1.npy',spec_pt_tst1)\n",
    "np.save('../../data/processed_data/spec_pt_timestretch1_flatten.npy',spec_pt_flat_tst1)\n",
    "np.save('../../data/processed_data/spec_pt_timestretch2.npy',spec_pt_tst2)\n",
    "np.save('../../data/processed_data/spec_pt_timestretch2_flatten.npy',spec_pt_flat_tst2)\n",
    "np.save('../../data/processed_data/spec_pt_timestretch3.npy',spec_pt_tst3)\n",
    "np.save('../../data/processed_data/spec_pt_timestretch3_flatten.npy',spec_pt_flat_tst3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0cc809d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data/processed_data/spec_pt_ftmask1.npy',ftmask1)\n",
    "np.save('../../data/processed_data/spec_pt_ftmask1_flatten.npy',ftmask1_flat)\n",
    "np.save('../../data/processed_data/spec_pt_ftmask2.npy',ftmask2)\n",
    "np.save('../../data/processed_data/spec_pt_ftmask2_flatten.npy',ftmask2_flat)\n",
    "np.save('../../data/processed_data/spec_pt_ftmask3.npy',ftmask3)\n",
    "np.save('../../data/processed_data/spec_pt_ftmask3_flatten.npy',ftmask3_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d645af1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
